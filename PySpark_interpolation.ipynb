{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c7d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext as sc, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql.types import IntegerType, StringType, LongType, FloatType, DoubleType, StructType, StructField, ArrayType\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import StandardScaler, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import functools \n",
    "import pytz\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from operator import attrgetter\n",
    "\n",
    "import argparse\n",
    "import boto3\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "import seaborn as sns\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\\\n",
    "         .config(conf=conf)\\\n",
    "         .config('fs.s3a.access.key', credentials.access_key)\\\n",
    "         .config('fs.s3a.secret.key', credentials.secret_key)\\\n",
    "         .appName('HRI')\\\n",
    "         .config(\"spark.executor.memory\", \"70g\")\\\n",
    "         .config(\"spark.driver.memory\", \"50g\")\\\n",
    "         .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "         .config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation\n",
    "\n",
    "max_ordnung = df2.agg({'ordnung': 'max'}).collect()[0].__getitem__('max(ordnung)')\n",
    "min_ordnung = df2.agg({'ordnung': 'min'}).collect()[0].__getitem__('min(ordnung)')\n",
    "step_size = round(((max_ordnung-min_ordnung)/(count_wert-1)),2)\n",
    "\n",
    "def ordnung_range(t1, t2, step=0.01):\n",
    "    \"\"\"Return a list of equally spaced points between t1 and t2 with stepsize step.\"\"\"\n",
    "    return np.round(np.arange(t1, t2, step) ,4).tolist()\n",
    "\n",
    "# define udf\n",
    "ordnung_range_udf = F.udf(ordnung_range, ArrayType(FloatType()))\n",
    "\n",
    "df_base_max  = df2.groupBy('DMC', 'spindle', 'processstep', 'TimeStamp').agg({\"ordnung\": \"max\"})\n",
    "df_base_min  = df2.groupBy('DMC', 'spindle', 'processstep', 'TimeStamp').agg({\"ordnung\": \"min\"})\n",
    "df_base  = df_base_max.join(df_base_min, on = ['DMC', 'spindle', 'processstep', 'TimeStamp'], how = 'leftouter')\n",
    "\n",
    "# # generate ordnung-grid and explode\n",
    "\n",
    "df_base = df_base.withColumn(\"ordnung\", F.explode(ordnung_range_udf(\"min(ordnung)\", \"max(ordnung)\")))\n",
    "df2 = df2.withColumn('ordnung_copy', F.col('ordnung'))\n",
    "df_all_ord = df_base.join(df2, ['DMC', 'spindle', 'processstep', 'ordnung', 'TimeStamp'], \"outer\")\n",
    "\n",
    "\n",
    "window_ff = Window.partitionBy('DMC', 'spindle', 'processstep', 'TimeStamp')\\\n",
    "               .orderBy('ordnung')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "               \n",
    "window_bf = Window.partitionBy('DMC', 'spindle', 'processstep', 'TimeStamp')\\\n",
    "               .orderBy('ordnung')\\\n",
    "               .rowsBetween(0, sys.maxsize)\n",
    "\n",
    "# create the series containing the filled values\n",
    "value_last = F.last(df_all_ord['Value'], ignorenulls=True).over(window_ff)\n",
    "ordnung_last = F.last(df_all_ord['ordnung_copy'], ignorenulls=True).over(window_ff)\n",
    "\n",
    "value_next = F.first(df_all_ord['Value'], ignorenulls=True).over(window_bf)\n",
    "readtime_next = F.first(df_all_ord['ordnung_copy'], ignorenulls=True).over(window_bf)\n",
    "\n",
    "# add the columns to the dataframe\n",
    "df_filled = df_all_ord.withColumn('Value_ff', value_last)\\\n",
    "                      .withColumn('ordnung_ff', ordnung_last)\\\n",
    "                      .withColumn('Value_bf', value_next)\\\n",
    "                      .withColumn('ordnung_bf', readtime_next)\n",
    "\n",
    "# define interpolation function\n",
    "def interpol(x, x_prev, x_next, y_prev, y_next, y):\n",
    "    if x_prev == x_next:\n",
    "        return y\n",
    "    else:\n",
    "        m = (y_next-y_prev)/(x_next-x_prev)\n",
    "        y_interpol = y_prev + m * (x - x_prev)\n",
    "        return y_interpol\n",
    "    \n",
    "interpol_udf = F.udf(interpol, FloatType())  \n",
    "\n",
    "# add interpolated columns to dataframe and clean up\n",
    "\n",
    "df_filled = df_filled.withColumn('value_interpol', F.when(F.col('ordnung_ff') == F.col('ordnung_bf'), F.col('Value'))\\\n",
    "                                                   .otherwise((((((F.col('Value_bf')-F.col('Value_ff'))/(F.col('ordnung_bf')-F.col('ordnung_ff')))*(F.col('ordnung')-F.col('ordnung_ff')))+F.col('Value_ff')))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
